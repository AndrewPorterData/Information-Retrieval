{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18be7068-5a9a-4343-b7c9-74d5962e6d59",
   "metadata": {},
   "source": [
    "### Spherical K-means as Efficiency Improvement for Vector Space Model\n",
    "\n",
    "Spherical K-means is a widely used adaptation of the K-means algorithm, modified to cluster sparse high-dimensional data objects (i.e. text data). This code was written as a practical application of ideas covered by various academics over recent decades. Spherical K-means is used here as an extension to a vector space model search engine, with the goal of boosting search efficiency. By narrowing the search scope to the most relevant cluster (i.e. topic cluster). It reduces the number of document comparisons needed to find relevant articles, potentially improving both speed and relevance for large datasets. The dataset used is a reduced version of a json file holding roughly 19000 wikipedia articles (text, ids, titles etc.).\n",
    "\n",
    "This code was used on the dataset with roughly 1900 articles, could easily be expanded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad3e03-661e-426d-9694-6552e9c03e53",
   "metadata": {},
   "source": [
    "### KMeans Tweak For Cosine Similarity:\n",
    "\n",
    "KMeans clustering with normalisation is used so that the euclidean distance calculation is more akin to cosine similarity. This normalisation step ensures that the document vectors are on a unit sphere, making the Euclidean distance between them more reflective of the angle between vectors. The clustering now indirectly attempts to minimise angular differences rather than euclidean distances. The initial clustering process now also focusses on the direction (semantics) rather than magnitude (length or frequency). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c719f071-f107-4791-abb7-f1206289046a",
   "metadata": {},
   "source": [
    "#### Loading and Preprocessing:\n",
    "\n",
    "Loading: This function loads the text data from a compressed (zip) file containing JSON objects. Each JSON object represents a Wikipedia article.\n",
    "\n",
    "Preprocessing: While this code snippet primarily focuses on loading the articles, in a full implementation, preprocessing would typically involve cleaning the text (e.g., removing punctuation, lowercasing, etc.) and possibly tokenizing or lemmatizing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69b6394a-4b57-4fcc-bf3b-2d964d39be46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import zipfile\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess(file_path):\n",
    "    articles_text = []\n",
    "    articles_titles = []  # Store article titles\n",
    "    with zipfile.ZipFile(file_path, 'r') as z:\n",
    "        with z.open(z.namelist()[0]) as f:\n",
    "            for line in f:\n",
    "                article = json.loads(line)\n",
    "                # Combine the title and text\n",
    "                combined_text = article['title'] + \" \" + article['text']\n",
    "                articles_text.append(combined_text)  # Use combined text for TF-IDF\n",
    "                articles_titles.append(article['title'])  # Keep titles for later retrieval\n",
    "    return articles_text, articles_titles\n",
    "\n",
    "\n",
    "articles_text, articles_titles = load_and_preprocess('/Users/andrew/Documents/Programming For AI&Data/wiki-articles-small.json.zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0074b4-6ec9-4933-84d9-7eafc08701d9",
   "metadata": {},
   "source": [
    "#### Vectorizing the Text with TF-IDF\n",
    "\n",
    "TF-IDF Vectorization: Converts the preprocessed text documents into a matrix of TF-IDF features. It transforms the text into a numerical representation that reflects how important a word is to a document in the context of the entire corpus. Stop words (commonly used words of little value) are removed to improve the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2510d946-8f2d-4ad2-ae93-774806159a11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(articles_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c95509-050c-4fd1-94a4-e9c39ba6af42",
   "metadata": {},
   "source": [
    "#### Normalize the TF-IDF Vectors\n",
    "\n",
    "Normalization: Adjusts the TF-IDF vectors so that each vector has a magnitude (or length) of 1. This is done by scaling the vector components such that the vector lies on a unit sphere. This step ensures that the distance calculations in the subsequent clustering phase are more reflective of the angle between vectors, aligning better with the concept of cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cc7807b-8cf9-4955-a337-c165bbd64789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "normalized_vectors = normalize(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de712c2-041b-48e2-bc17-e9abb7a3051f",
   "metadata": {},
   "source": [
    "#### Cluster the Documents Using K-means\n",
    "\n",
    "Clustering: Applies the K-means algorithm to the normalized TF-IDF vectors to group documents into clusters based on their similarities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28a2a66d-46f8-4303-bba5-2aab6d817a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "kmeans.fit(normalized_vectors)\n",
    "clusters = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f5f12e-f68e-4406-9663-ed90199a176b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Creates an Index Mapping Clusters to Articles\n",
    "\n",
    "Index Creation: Constructs a mapping from clusters to document indices, facilitating the retrieval of documents belonging to a specific cluster. This index is crucial for efficiently finding relevant documents during the search process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b378f0fc-f651-4aa5-a0ec-bd4b2c5adc1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_cluster_index(clusters):\n",
    "    cluster_index = {}\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        if cluster not in cluster_index:\n",
    "            cluster_index[cluster] = []\n",
    "        cluster_index[cluster].append(i)\n",
    "    return cluster_index\n",
    "\n",
    "cluster_index = create_cluster_index(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c05c8-6646-4c2e-b29f-5bda30968dd3",
   "metadata": {},
   "source": [
    "#### Process a Search Query\n",
    "\n",
    "Implements a search function that identifies the most relevant cluster to a query, calculates the cosine similarity within that cluster, and returns the titles of the top N most similar articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc1869c7-36b8-43e3-83c7-8ae5e6f1d6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the search function\n",
    "def search(query, tfidf_vectorizer, kmeans_model, normalized_vectors, cluster_index, articles_titles, top_n=5):\n",
    "    \n",
    "    query = query.lower() # Note, the tf-idf vectorizer converts tokens to lowercase by default\n",
    "    \n",
    "    # Vectorize and normalize query\n",
    "    query_vector = tfidf_vectorizer.transform([query])\n",
    "    normalized_query_vector = normalize(query_vector)\n",
    "    \n",
    "    # Identify the most relevant cluster\n",
    "    cluster_centroids = kmeans_model.cluster_centers_\n",
    "    query_cluster_similarities = cosine_similarity(normalized_query_vector, cluster_centroids)\n",
    "    most_relevant_cluster = query_cluster_similarities.argmax()\n",
    "    \n",
    "    # Find the most similar documents within the cluster\n",
    "    docs_in_relevant_cluster = cluster_index[most_relevant_cluster]\n",
    "    doc_similarities = cosine_similarity(normalized_query_vector, normalized_vectors[docs_in_relevant_cluster]).flatten()\n",
    "    most_similar_docs_indices = np.argsort(doc_similarities)[-top_n:][::-1]\n",
    "    \n",
    "    # Returns most similar titles, can be altered to return text instead\n",
    "    most_similar_titles = [articles_titles[docs_in_relevant_cluster[idx]] for idx in most_similar_docs_indices]\n",
    "    return most_similar_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2539553-5886-4409-a61a-a7458f814a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeline of programming languages\n",
      "Lynx (programming language)\n",
      "Programmer\n",
      "Niklaus Wirth\n",
      "Design Patterns\n",
      "Recursion\n",
      "Software\n",
      "Prolog\n",
      "Assembly language\n",
      "Self-reference\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"Python Programming\"\n",
    "top_titles = search(query, tfidf_vectorizer, kmeans, normalized_vectors, cluster_index, articles_titles, top_n=10)\n",
    "\n",
    "for title in top_titles:\n",
    "    print(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef503b8-7147-4a6d-b8aa-625b8f09baaa",
   "metadata": {},
   "source": [
    "#### Full Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eddb5491-504f-4069-87a1-8fc9c51790df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeline of programming languages\n",
      "Lynx (programming language)\n",
      "Programmer\n",
      "Niklaus Wirth\n",
      "Design Patterns\n",
      "Recursion\n",
      "Software\n",
      "Prolog\n",
      "Assembly language\n",
      "Self-reference\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import zipfile\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load articles and preprocess\n",
    "def load_and_preprocess(file_path):\n",
    "    articles_text = []\n",
    "    articles_titles = []  # Store article titles\n",
    "    with zipfile.ZipFile(file_path, 'r') as z:\n",
    "        with z.open(z.namelist()[0]) as f:\n",
    "            for line in f:\n",
    "                article = json.loads(line)\n",
    "                # Combine the title and text\n",
    "                combined_text = article['title'] + \" \" + article['text']\n",
    "                articles_text.append(combined_text)  # Use combined text for TF-IDF\n",
    "                articles_titles.append(article['title'])  # Keep titles for later retrieval\n",
    "    return articles_text, articles_titles\n",
    "\n",
    "\n",
    "articles_text, articles_titles = load_and_preprocess('/Users/andrew/Documents/Programming For AI&Data/wiki-articles-small.json.zip')\n",
    "\n",
    "# Step 2: Vectorize the combined text and titles using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(articles_text)\n",
    "\n",
    "# Normalize the TF-IDF vectors\n",
    "normalized_vectors = normalize(tfidf_matrix)\n",
    "\n",
    "# Step 3: Cluster the documents using K-means\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "kmeans.fit(normalized_vectors)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# Create an index mapping clusters to articles\n",
    "def create_cluster_index(clusters):\n",
    "    cluster_index = {}\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        if cluster not in cluster_index:\n",
    "            cluster_index[cluster] = []\n",
    "        cluster_index[cluster].append(i)\n",
    "    return cluster_index\n",
    "\n",
    "cluster_index = create_cluster_index(clusters)\n",
    "\n",
    "# Step 4: Define the search function\n",
    "def search(query, tfidf_vectorizer, kmeans_model, normalized_vectors, cluster_index, articles_titles, top_n=5):\n",
    "    \n",
    "    query = query.lower()\n",
    "    \n",
    "    # Vectorize and normalize query\n",
    "    query_vector = tfidf_vectorizer.transform([query])\n",
    "    normalized_query_vector = normalize(query_vector)\n",
    "    \n",
    "    # Identify the most relevant cluster\n",
    "    cluster_centroids = kmeans_model.cluster_centers_\n",
    "    query_cluster_similarities = cosine_similarity(normalized_query_vector, cluster_centroids)\n",
    "    most_relevant_cluster = query_cluster_similarities.argmax()\n",
    "    \n",
    "    # Find the most similar documents within the cluster\n",
    "    docs_in_relevant_cluster = cluster_index[most_relevant_cluster]\n",
    "    doc_similarities = cosine_similarity(normalized_query_vector, normalized_vectors[docs_in_relevant_cluster]).flatten()\n",
    "    most_similar_docs_indices = np.argsort(doc_similarities)[-top_n:][::-1]\n",
    "    \n",
    "    # Returns most similar titles, can be altered to return text instead\n",
    "    most_similar_titles = [articles_titles[docs_in_relevant_cluster[idx]] for idx in most_similar_docs_indices]\n",
    "    return most_similar_titles\n",
    "\n",
    "# Example usage\n",
    "query = \"Python Programming\"\n",
    "top_titles = search(query, tfidf_vectorizer, kmeans, normalized_vectors, cluster_index, articles_titles, top_n=10)\n",
    "\n",
    "for title in top_titles:\n",
    "    print(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e307622-f241-45c2-ac2c-624283b7dac6",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "Hornik, K., Feinerer, I., Kober, M. and Buchta, C., 2012. Spherical k-means clustering. Journal of statistical software, 50, pp.1-22.\n",
    "\n",
    "Huang, A., 2008, April. Similarity measures for text document clustering. In Proceedings of the sixth new zealand computer science research student conference (NZCSRSC2008), Christchurch, New Zealand (Vol. 4, pp. 9-56).\n",
    "\n",
    "Zhong, S., 2005, July. Efficient online spherical k-means clustering. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005. (Vol. 5, pp. 3180-3185). IEEE.\n",
    "\n",
    "-----\n",
    "\n",
    "Important quotes for choice of K-means: \n",
    "\n",
    "Huang:\n",
    "\n",
    "\"Partitional clustering algorithms have been recognized to be better suited for handling large document datasets than hi- erarchical ones, due to their relatively low computational requirements [16, 9, 3].\" p.52\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
